---
date: 2025-10-11T06:57:40+09:00
title: "Python ë¹„ë™ê¸° í”„ë¡œê·¸ë˜ë°, async/awaitë¡œ ì„±ëŠ¥ 10ë°° ëŒì–´ì˜¬ë¦¬ê¸°"
description: "Pythonì˜ async/awaitë¥¼ í™œìš©í•œ ë¹„ë™ê¸° í”„ë¡œê·¸ë˜ë° ì™„ë²½ ê°€ì´ë“œì…ë‹ˆë‹¤. ê¸°ë³¸ ë¬¸ë²•ë¶€í„° ì‹¤ì „ ì›¹ í¬ë¡¤ë§, ìì£¼ í•˜ëŠ” ì‹¤ìˆ˜ì™€ ì„±ëŠ¥ ìµœì í™”ê¹Œì§€ ì‹¤ìš©ì ì¸ ì˜ˆì œì™€ í•¨ê»˜ ì„¤ëª…í•©ë‹ˆë‹¤."
categories:
  - Programming
tags:
  - Python
  - ë¹„ë™ê¸°í”„ë¡œê·¸ë˜ë°
  - asyncio
  - async/await
  - aiohttp
---

> Pythonì˜ async/awaitë¥¼ í™œìš©í•œ ë¹„ë™ê¸° í”„ë¡œê·¸ë˜ë° ì™„ë²½ ê°€ì´ë“œì…ë‹ˆë‹¤. ê¸°ë³¸ ë¬¸ë²•ë¶€í„° ì‹¤ì „ ì›¹ í¬ë¡¤ë§, ìì£¼ í•˜ëŠ” ì‹¤ìˆ˜ì™€ ì„±ëŠ¥ ìµœì í™”ê¹Œì§€ ì‹¤ìš©ì ì¸ ì˜ˆì œì™€ í•¨ê»˜ ì„¤ëª…í•©ë‹ˆë‹¤.



<!-- more -->

## ë¹„ë™ê¸° í”„ë¡œê·¸ë˜ë°, ì™œ í•„ìš”í• ê¹Œìš”?

ì›¹ í¬ë¡¤ë§ì„ í•˜ê±°ë‚˜ API ìš”ì²­ì„ ì—¬ëŸ¬ ë²ˆ ë³´ë‚¼ ë•Œ, í”„ë¡œê·¸ë¨ì´ í•œì°¸ ë™ì•ˆ ë©ˆì¶°ìˆëŠ” ê²½í—˜ í•´ë³´ì…¨ë‚˜ìš”? ì´ê±´ ë™ê¸°(Synchronous) ë°©ì‹ìœ¼ë¡œ ì‘ë™í•˜ê¸° ë•Œë¬¸ì¸ë°ìš”. í•œ ì‘ì—…ì´ ëë‚  ë•Œê¹Œì§€ ë‹¤ìŒ ì‘ì—…ì„ ê¸°ë‹¤ë ¤ì•¼ í•˜ì£ .

ë¹„ë™ê¸° í”„ë¡œê·¸ë˜ë°ì€ ì´ëŸ° 'ëŒ€ê¸° ì‹œê°„'ì„ íš¨ìœ¨ì ìœ¼ë¡œ í™œìš©í•©ë‹ˆë‹¤. ë„¤íŠ¸ì›Œí¬ ì‘ë‹µì„ ê¸°ë‹¤ë¦¬ëŠ” ë™ì•ˆ ë‹¤ë¥¸ ì‘ì—…ì„ ì²˜ë¦¬í•  ìˆ˜ ìˆì–´, íŠ¹íˆ I/O ì‘ì—…ì´ ë§ì€ í”„ë¡œê·¸ë¨ì—ì„œ ì„±ëŠ¥ì´ ê·¹ì ìœ¼ë¡œ í–¥ìƒë©ë‹ˆë‹¤. ì‹¤ì œë¡œ 100ê°œì˜ ì›¹í˜ì´ì§€ë¥¼ í¬ë¡¤ë§í•  ë•Œ, ë™ê¸° ë°©ì‹ì€ 100ì´ˆê°€ ê±¸ë¦¬ì§€ë§Œ ë¹„ë™ê¸° ë°©ì‹ì€ 10ì´ˆ ì´ë‚´ë¡œ ì™„ë£Œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## async/await ê¸°ë³¸ ë¬¸ë²• ë§ˆìŠ¤í„°í•˜ê¸°

Python 3.5ë¶€í„° ë„ì…ëœ async/awaitëŠ” ë¹„ë™ê¸° ì½”ë“œë¥¼ ë§ˆì¹˜ ë™ê¸° ì½”ë“œì²˜ëŸ¼ ì§ê´€ì ìœ¼ë¡œ ì‘ì„±í•  ìˆ˜ ìˆê²Œ í•´ì¤ë‹ˆë‹¤. ê¸°ë³¸ êµ¬ì¡°ë¶€í„° ì‚´í´ë³¼ê¹Œìš”?

python
import asyncio

async def fetch_data(url):
    print(f"{url} ìš”ì²­ ì‹œì‘")
    await asyncio.sleep(2)  # ë„¤íŠ¸ì›Œí¬ ìš”ì²­ ì‹œë®¬ë ˆì´ì…˜
    print(f"{url} ì™„ë£Œ")
    return f"{url}ì˜ ë°ì´í„°"

async def main():
    # ì—¬ëŸ¬ ì‘ì—…ì„ ë™ì‹œì— ì‹¤í–‰
    tasks = [
        fetch_data("https://api1.com"),
        fetch_data("https://api2.com"),
        fetch_data("https://api3.com")
    ]
    results = await asyncio.gather(*tasks)
    return results

# ì‹¤í–‰
results = asyncio.run(main())


**í•µì‹¬ í¬ì¸íŠ¸**: `async def`ë¡œ ì½”ë£¨í‹´(Coroutine) í•¨ìˆ˜ë¥¼ ì •ì˜í•˜ê³ , `await` í‚¤ì›Œë“œë¡œ ë¹„ë™ê¸° ì‘ì—…ì´ ì™„ë£Œë  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦½ë‹ˆë‹¤. `asyncio.gather()`ëŠ” ì—¬ëŸ¬ ì‘ì—…ì„ ë™ì‹œì— ì‹¤í–‰í•˜ê³  ëª¨ë“  ê²°ê³¼ë¥¼ ëª¨ì•„ì¤ë‹ˆë‹¤.

## ì‹¤ì „ í™œìš©: aiohttpë¡œ ì›¹ í¬ë¡¤ë§í•˜ê¸°

ì‹¤ë¬´ì—ì„œ ê°€ì¥ ë§ì´ ì‚¬ìš©í•˜ëŠ” íŒ¨í„´ì€ aiohttpë¥¼ í™œìš©í•œ ë¹„ë™ê¸° HTTP ìš”ì²­ì…ë‹ˆë‹¤. requests ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” ë™ê¸° ë°©ì‹ì´ë¼ ë¹„ë™ê¸° í™˜ê²½ì—ì„œëŠ” aiohttpë¥¼ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.

python
import aiohttp
import asyncio
import time

async def fetch_page(session, url):
    async with session.get(url) as response:
        return await response.text()

async def crawl_websites(urls):
    async with aiohttp.ClientSession() as session:
        tasks = [fetch_page(session, url) for url in urls]
        pages = await asyncio.gather(*tasks)
        return pages

# ì„±ëŠ¥ ë¹„êµ
urls = [f"https://example.com/page{i}" for i in range(50)]

start = time.time()
results = asyncio.run(crawl_websites(urls))
print(f"ë¹„ë™ê¸° ë°©ì‹: {time.time() - start:.2f}ì´ˆ")


**ğŸ’¡ ì‹¤ìš© íŒ**: `ClientSession`ì€ ì—°ê²°ì„ ì¬ì‚¬ìš©í•˜ë¯€ë¡œ ë°˜ë“œì‹œ í•˜ë‚˜ì˜ ì„¸ì…˜ì„ ì—¬ëŸ¬ ìš”ì²­ì—ì„œ ê³µìœ í•˜ì„¸ìš”. ë§¤ë²ˆ ìƒˆë¡œ ë§Œë“¤ë©´ ì„±ëŠ¥ ì´ì ì´ ì‚¬ë¼ì§‘ë‹ˆë‹¤.

## ìì£¼ ë§Œë‚˜ëŠ” í•¨ì •ê³¼ í•´ê²°ë²•

ë¹„ë™ê¸° í”„ë¡œê·¸ë˜ë°ì„ ì‹œì‘í•˜ë©´ ëª‡ ê°€ì§€ í”í•œ ì‹¤ìˆ˜ë¥¼ í•˜ê²Œ ë©ë‹ˆë‹¤. ì œê°€ ê²½í—˜í•œ ì£¼ìš” ì´ìŠˆë“¤ì„ ê³µìœ í• ê²Œìš”.

**1. Blocking í•¨ìˆ˜ ì‚¬ìš©í•˜ê¸°**

python
# âŒ ì˜ëª»ëœ ì˜ˆ
async def bad_example():
    time.sleep(1)  # ì „ì²´ ì´ë²¤íŠ¸ ë£¨í”„ë¥¼ ë§‰ì•„ë²„ë¦¼!

# âœ… ì˜¬ë°”ë¥¸ ì˜ˆ
async def good_example():
    await asyncio.sleep(1)  # ë‹¤ë¥¸ ì‘ì—…ì´ ì‹¤í–‰ ê°€ëŠ¥


**2. await ì—†ì´ ì½”ë£¨í‹´ í˜¸ì¶œ**

python
# âŒ ì˜ëª»ëœ ì˜ˆ
async def wrong():
    fetch_data("url")  # ì‹¤í–‰ë˜ì§€ ì•ŠìŒ!

# âœ… ì˜¬ë°”ë¥¸ ì˜ˆ
async def correct():
    await fetch_data("url")


**3. ì˜ˆì™¸ ì²˜ë¦¬ ëˆ„ë½**

ë¹„ë™ê¸° ì‘ì—…ì—ì„œ ì˜ˆì™¸ê°€ ë°œìƒí•˜ë©´ ì¡°ìš©íˆ ë¬´ì‹œë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. `asyncio.gather()`ì˜ `return_exceptions=True` ì˜µì…˜ì„ í™œìš©í•˜ì„¸ìš”.

python
async def safe_gather():
    results = await asyncio.gather(
        *tasks,
        return_exceptions=True
    )
    for result in results:
        if isinstance(result, Exception):
            print(f"ì—ëŸ¬ ë°œìƒ: {result}")


## ì„±ëŠ¥ ìµœì í™”ì™€ ëª¨ë‹ˆí„°ë§

ë¹„ë™ê¸° í”„ë¡œê·¸ë¨ì˜ ì„±ëŠ¥ì„ ê·¹ëŒ€í™”í•˜ë ¤ë©´ ë™ì‹œ ì‹¤í–‰ ì‘ì—… ìˆ˜ë¥¼ ì ì ˆíˆ ì œí•œí•´ì•¼ í•©ë‹ˆë‹¤. ë¬´í•œì • ëŠ˜ë¦¬ë©´ ì˜¤íˆë ¤ ë¦¬ì†ŒìŠ¤ ê³ ê°ˆë¡œ ì„±ëŠ¥ì´ ë–¨ì–´ì§‘ë‹ˆë‹¤.

python
async def limited_crawl(urls, limit=10):
    semaphore = asyncio.Semaphore(limit)
    
    async def fetch_with_limit(url):
        async with semaphore:
            return await fetch_page(session, url)
    
    async with aiohttp.ClientSession() as session:
        tasks = [fetch_with_limit(url) for url in urls]
        return await asyncio.gather(*tasks)


**Semaphore**ëŠ” ë™ì‹œ ì‹¤í–‰ ê°œìˆ˜ë¥¼ ì œí•œí•˜ëŠ” ì¹´ìš´í„°ì…ë‹ˆë‹¤. ì›¹ ì„œë²„ì— ê³¼ë¶€í•˜ë¥¼ ì£¼ì§€ ì•Šìœ¼ë©´ì„œë„ ìµœì ì˜ ì„±ëŠ¥ì„ ë‚¼ ìˆ˜ ìˆì£ .

**ë””ë²„ê¹… íŒ**: `PYTHONASYNICDEBUG=1` í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ë©´ awaitë˜ì§€ ì•Šì€ ì½”ë£¨í‹´ì„ ê²½ê³ í•´ì¤ë‹ˆë‹¤. ê°œë°œ ì¤‘ì—ëŠ” í•„ìˆ˜ë¡œ ì¼œë‘ì„¸ìš”!

ë¹„ë™ê¸° í”„ë¡œê·¸ë˜ë°ì€ ì²˜ìŒì—” ì–´ë µê²Œ ëŠê»´ì§€ì§€ë§Œ, ê¸°ë³¸ íŒ¨í„´ë§Œ ìµíˆë©´ ê°•ë ¥í•œ ë„êµ¬ê°€ ë©ë‹ˆë‹¤. íŠ¹íˆ API ì„œë²„, ì›¹ í¬ë¡¤ëŸ¬, ë°ì´í„° íŒŒì´í”„ë¼ì¸ ë“±ì—ì„œ ê·¸ ì§„ê°€ë¥¼ ë°œíœ˜í•˜ë‹ˆ ê¼­ ë§ˆìŠ¤í„°í•´ë³´ì„¸ìš”!

---

*ì´ ê¸€ì€ AIê°€ ìë™ìœ¼ë¡œ ì‘ì„±í–ˆìŠµë‹ˆë‹¤.*
