---
date: 2025-10-11T06:57:40+09:00
title: "Python 비동기 프로그래밍, async/await로 성능 10배 끌어올리기"
description: "Python의 async/await를 활용한 비동기 프로그래밍 완벽 가이드입니다. 기본 문법부터 실전 웹 크롤링, 자주 하는 실수와 성능 최적화까지 실용적인 예제와 함께 설명합니다."
categories:
  - Programming
tags:
  - Python
  - 비동기프로그래밍
  - asyncio
  - async/await
  - aiohttp
---

> Python의 async/await를 활용한 비동기 프로그래밍 완벽 가이드입니다. 기본 문법부터 실전 웹 크롤링, 자주 하는 실수와 성능 최적화까지 실용적인 예제와 함께 설명합니다.



<!-- more -->

## 비동기 프로그래밍, 왜 필요할까요?

웹 크롤링을 하거나 API 요청을 여러 번 보낼 때, 프로그램이 한참 동안 멈춰있는 경험 해보셨나요? 이건 동기(Synchronous) 방식으로 작동하기 때문인데요. 한 작업이 끝날 때까지 다음 작업을 기다려야 하죠.

비동기 프로그래밍은 이런 '대기 시간'을 효율적으로 활용합니다. 네트워크 응답을 기다리는 동안 다른 작업을 처리할 수 있어, 특히 I/O 작업이 많은 프로그램에서 성능이 극적으로 향상됩니다. 실제로 100개의 웹페이지를 크롤링할 때, 동기 방식은 100초가 걸리지만 비동기 방식은 10초 이내로 완료할 수 있습니다.

## async/await 기본 문법 마스터하기

Python 3.5부터 도입된 async/await는 비동기 코드를 마치 동기 코드처럼 직관적으로 작성할 수 있게 해줍니다. 기본 구조부터 살펴볼까요?

python
import asyncio

async def fetch_data(url):
    print(f"{url} 요청 시작")
    await asyncio.sleep(2)  # 네트워크 요청 시뮬레이션
    print(f"{url} 완료")
    return f"{url}의 데이터"

async def main():
    # 여러 작업을 동시에 실행
    tasks = [
        fetch_data("https://api1.com"),
        fetch_data("https://api2.com"),
        fetch_data("https://api3.com")
    ]
    results = await asyncio.gather(*tasks)
    return results

# 실행
results = asyncio.run(main())


**핵심 포인트**: `async def`로 코루틴(Coroutine) 함수를 정의하고, `await` 키워드로 비동기 작업이 완료될 때까지 기다립니다. `asyncio.gather()`는 여러 작업을 동시에 실행하고 모든 결과를 모아줍니다.

## 실전 활용: aiohttp로 웹 크롤링하기

실무에서 가장 많이 사용하는 패턴은 aiohttp를 활용한 비동기 HTTP 요청입니다. requests 라이브러리는 동기 방식이라 비동기 환경에서는 aiohttp를 사용해야 합니다.

python
import aiohttp
import asyncio
import time

async def fetch_page(session, url):
    async with session.get(url) as response:
        return await response.text()

async def crawl_websites(urls):
    async with aiohttp.ClientSession() as session:
        tasks = [fetch_page(session, url) for url in urls]
        pages = await asyncio.gather(*tasks)
        return pages

# 성능 비교
urls = [f"https://example.com/page{i}" for i in range(50)]

start = time.time()
results = asyncio.run(crawl_websites(urls))
print(f"비동기 방식: {time.time() - start:.2f}초")


**💡 실용 팁**: `ClientSession`은 연결을 재사용하므로 반드시 하나의 세션을 여러 요청에서 공유하세요. 매번 새로 만들면 성능 이점이 사라집니다.

## 자주 만나는 함정과 해결법

비동기 프로그래밍을 시작하면 몇 가지 흔한 실수를 하게 됩니다. 제가 경험한 주요 이슈들을 공유할게요.

**1. Blocking 함수 사용하기**

python
# ❌ 잘못된 예
async def bad_example():
    time.sleep(1)  # 전체 이벤트 루프를 막아버림!

# ✅ 올바른 예
async def good_example():
    await asyncio.sleep(1)  # 다른 작업이 실행 가능


**2. await 없이 코루틴 호출**

python
# ❌ 잘못된 예
async def wrong():
    fetch_data("url")  # 실행되지 않음!

# ✅ 올바른 예
async def correct():
    await fetch_data("url")


**3. 예외 처리 누락**

비동기 작업에서 예외가 발생하면 조용히 무시될 수 있습니다. `asyncio.gather()`의 `return_exceptions=True` 옵션을 활용하세요.

python
async def safe_gather():
    results = await asyncio.gather(
        *tasks,
        return_exceptions=True
    )
    for result in results:
        if isinstance(result, Exception):
            print(f"에러 발생: {result}")


## 성능 최적화와 모니터링

비동기 프로그램의 성능을 극대화하려면 동시 실행 작업 수를 적절히 제한해야 합니다. 무한정 늘리면 오히려 리소스 고갈로 성능이 떨어집니다.

python
async def limited_crawl(urls, limit=10):
    semaphore = asyncio.Semaphore(limit)
    
    async def fetch_with_limit(url):
        async with semaphore:
            return await fetch_page(session, url)
    
    async with aiohttp.ClientSession() as session:
        tasks = [fetch_with_limit(url) for url in urls]
        return await asyncio.gather(*tasks)


**Semaphore**는 동시 실행 개수를 제한하는 카운터입니다. 웹 서버에 과부하를 주지 않으면서도 최적의 성능을 낼 수 있죠.

**디버깅 팁**: `PYTHONASYNICDEBUG=1` 환경변수를 설정하면 await되지 않은 코루틴을 경고해줍니다. 개발 중에는 필수로 켜두세요!

비동기 프로그래밍은 처음엔 어렵게 느껴지지만, 기본 패턴만 익히면 강력한 도구가 됩니다. 특히 API 서버, 웹 크롤러, 데이터 파이프라인 등에서 그 진가를 발휘하니 꼭 마스터해보세요!

---

*이 글은 AI가 자동으로 작성했습니다.*
